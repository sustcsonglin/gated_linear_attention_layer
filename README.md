# Gated Linear Attention Transformer Layer
Standalone module of Gated Linear Attention Transformer Layer (GLA) from [Gated Linear Attention Transformers with
Hardware-Efficient Training](https://arxiv.org/pdf/2312.06635.pdf) 

This repo will not be maintained. Just track some useful git commit. Please refer to [flash-linear-attention](https://github.com/sustcsonglin/flash-linear-attention).
 
## Requirement
torch
[triton nightly-release](https://github.com/openai/triton)




